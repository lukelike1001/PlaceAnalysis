{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **r/place 2023 Sentiment Analysis Model**\n",
    "This Jupyter notebook will fine-tune the DistilBERT model to perform sentiment analysis on Reddit comments in July 2023. Feel free to tweak the variables and code here. Credits are included at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Dependencies**<br>\n",
    "This notebook has been tested on Python 3.11.2 and uses Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Data**<br>\n",
    "The target CSV file has Reddit comments in Column 0 and a score in Column 1. The scores correspond to the following sentiments: -1 = negative, 0 = neutral, 1 = positive. We will tweak the range from [-1, 1] to [0, 2] to match the model's labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' family mormon have never tried explain them they still stare puzzled from time time like some kind strange creature nonetheless they have come admire for the patience calmness equanimity acceptance and compassion have developed all the things buddhism teaches ', 2)\n"
     ]
    }
   ],
   "source": [
    "# define the data path and store the comments in a list\n",
    "data_path = \"data/Reddit_Data.csv\"\n",
    "comments_and_scores = []\n",
    "\n",
    "# read the csv and store each comment with its respective score\n",
    "with open(data_path, \"r\", encoding=\"utf8\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        comment, score = row\n",
    "        comments_and_scores.append((comment, int(score)+1))\n",
    "\n",
    "print(comments_and_scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate Training and Testing Datasets**<br>\n",
    "We need to separate these comments into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(comments_and_scores,\n",
    "                                       test_size=0.2,\n",
    "                                       random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' yeh hindutva promote kar rahe hai ', 1)\n",
      "('vote for the best available candidate nota waste like you said ', 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0])\n",
    "print(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the training comments and scores\n",
    "train_comments = [group[0] for group in train_set]\n",
    "train_scores = [group[1] for group in train_set]\n",
    "\n",
    "# extract the testing comments and scores\n",
    "test_comments = [group[0] for group in test_set]\n",
    "test_scores = [group[1] for group in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yeh hindutva promote kar rahe hai  1\n",
      "vote for the best available candidate nota waste like you said  2\n"
     ]
    }
   ],
   "source": [
    "print(train_comments[0], train_scores[0])\n",
    "print(test_comments[0], test_scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training and testing datasets, we will convert them into Pandas DataFrame objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  score\n",
      "0       family mormon have never tried explain them t...      2\n",
      "1      buddhism has very much lot compatible with chr...      2\n",
      "2      seriously don say thing first all they won get...      0\n",
      "3      what you have learned yours and only yours wha...      1\n",
      "4      for your own benefit you may want read living ...      2\n",
      "...                                                  ...    ...\n",
      "37244                                              jesus      1\n",
      "37245  kya bhai pure saal chutiya banaya modi aur jab...      2\n",
      "37246              downvote karna tha par upvote hogaya       1\n",
      "37247                                         haha nice       2\n",
      "37248             facebook itself now working bjpâ€™ cell       1\n",
      "\n",
      "[37249 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_set = {\"text\": train_comments, \"score\": train_scores}\n",
    "train_set = pd.DataFrame(data)\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = {\"text\": test_comments, \"score\": test_scores}\n",
    "test_set = pd.DataFrame(data)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize the Data**<br>\n",
    "Prior to training the model, we will tokenize the Reddit comments into small pieces to make it easier for the model to identify the comment's sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the training and testing datasets\n",
    "# tokenized_train = [tokenizer(text) for train_set[\"text\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
