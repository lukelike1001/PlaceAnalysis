{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **r/place 2023 Sentiment Analysis Model Creation**\n",
    "\n",
    "In this Jupyter notebook, I will perform transfer learning to improve a BERT model using a dataset of Reddit comments and their sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits**<br>\n",
    "* Thank you to Professor Alvin Chen of the National Taiwan Normal University for his guide to performing transfer learning with BERT using IMDB movie reviews. The tutorial can be found at: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/temp/sentiment-analysis-using-bert-keras-movie-reviews.html\n",
    "\n",
    "* Thank you to Chaithanya Kumar A for collecting Reddit comments with associated sentiment values for this project. His dataset can be found at: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**<br>\n",
    "To create the NLP model, I use various libraries such as tensorflow, nltk, pandas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import torch\n",
    "import transformers\n",
    "import unicodedata\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Data**<br>\n",
    "The target CSV file has Reddit comments in Column 0 and a score in Column 1. The scores correspond to the following sentiments: -1 = negative, 0 = neutral, 1 = positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the data path and store the comments in a list\n",
    "data_path = \"data/Reddit_Data.csv\"\n",
    "comments = []\n",
    "\n",
    "# read the csv and store each comment with its respective score\n",
    "with open(data_path, \"r\", encoding=\"utf8\") as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        comment, score = row\n",
    "        comments.append((comment, int(score)))\n",
    "\n",
    "# shuffle the data for variety\n",
    "random.shuffle(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the Data**<br>\n",
    "I'll use a split ratio of 80:20 for this model. Feel free to tweak the ratio if you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(comments,\n",
    "                                       test_size=0.2,\n",
    "                                       random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting X and Y**<br>\n",
    "In this model, X = Reddit Comments, and Y = Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the maximum token size\n",
    "TOKEN_SIZE = 128\n",
    "\n",
    "# fill in the comments and scores of the training set\n",
    "x_train = []; y_train = []\n",
    "for comment, score in train_set:\n",
    "    if len(comment) > TOKEN_SIZE:\n",
    "        x_train.append(comment[:TOKEN_SIZE])\n",
    "    else:\n",
    "        x_train.append(comment)\n",
    "    y_train.append(int(score)+1)\n",
    "\n",
    "# fill in the comments and scores of the testing set\n",
    "x_test = []; y_test = []\n",
    "for comment, score in test_set:\n",
    "    if len(comment) > TOKEN_SIZE:\n",
    "        x_test.append(comment[:TOKEN_SIZE])\n",
    "    else:\n",
    "        x_test.append(comment)\n",
    "    y_test.append(int(score)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing the Data**<br>\n",
    "We will tokenize the data to make sure that the model can accept the string inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "# load the bertweet tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "# tokenize the reddit comments\n",
    "tokenized_train = tokenizer(x_train, return_tensors=\"np\", padding=True)\n",
    "tokenized_test = tokenizer(x_test, return_tensors=\"np\", padding=True)\n",
    "\n",
    "# convert \n",
    "labels_train = np.array(y_train)\n",
    "labels_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0   101    33    94  1498  8225  6010   116  2153   235 11108   136\n",
      "    33 12738   130   175  1029   153     6  9778 27674   520 22282     2\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing Data (for the BERT Model)**<br>\n",
    "Now that we have extracted the training and testing data, we can move onto converting the data so that the BERT model can take these inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load the bertweet model\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"finiteautomata/bertweet-base-sentiment-analysis\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store callbacks into a csv file\n",
    "filename='data/callbacks_log.csv'\n",
    "history_logger=tf.keras.callbacks.CSVLogger(filename, separator=\",\", append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/932 [===>..........................] - ETA: 8:45:24 - loss: 0.8283 - accuracy: 0.6738"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "history = model.fit(\n",
    "  dict(tokenized_train), \n",
    "  labels_train,\n",
    "  validation_data=(dict(tokenized_test), labels_test),\n",
    "  epochs=epochs,\n",
    "  callbacks=history_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 150\n",
    "\n",
    "\n",
    "# Plotting results\n",
    "def plot1(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    ## Accuracy plot\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    ## Loss plot\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot2(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    #plt.gca().set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
